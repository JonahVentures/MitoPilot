plugins {
  id 'nf-amazon'
	id 'nf-sqldb@0.5.0'
}

docker {
  enabled = true
}

process {
  executor = 'local'
  container = '<<CONTAINER_ID>>'
  queue = '<<BATCH_QUEUE>>'
  cpus = { opts.cpus }
  memory = { opts.memory.GB * task.attempt}
}

// AWS Batch parameters (modify as needed, see: https://www.nextflow.io/docs/latest/aws.html#aws-batch)
aws {
  region = '<<AWS_REGION>>'
  batch {
    cliPath = '<<AWS_CLI_PATH>>' // Path to the AWS CLI on your custom AMI
    maxParallelTransfers = 8
  }
  client {
    maxConnections = 16
  }
}

// SQL database configuration (https://github.com/nextflow-io/nf-sqldb)
sql {
  db {
    sqlite {
      url = 'jdbc:sqlite:.sqlite?journal_mode=WAL&busy_timeout=5000'
    }
  }
}

// Pipeline parameters
params {
    rawDir = '<<RAW_DIR>>'                  // Directory (or s3 bucket) containing raw data
    bucket-dir = '<<bucket-dir>>'           // Path to s3 bucket for stroing intermediate data
    publishDir = 'out'                      // Directory (or s3 bucket) to publish pipeline outputs
    minDepth = <<MIN_DEPTH>>                // Skip samples with less than this read depth
    minAssemblyLength = 500                 // min contig size to retain for annotation
    genetic_code = <<GENETIC_CODE>>
    mitos_condaenv = 'mitos'
    trnaScan_condaenv = 'trnascan-se'
    preprocess {
        container = process.container
        executor = process.executor
    }
    assemble {
        maxRetries = 1
        container = process.container
        executor = process.executor
    }
    coverage {
        cpus = 4
        memory = 8
        container = process.container
        executor = process.executor
    }
    annotate {
        container = process.container
        executor = process.executor
    }
    curate {
        container = process.container
        executor = process.executor
    }
    validate {
        container = process.container
        executor = process.executor
    }
}
